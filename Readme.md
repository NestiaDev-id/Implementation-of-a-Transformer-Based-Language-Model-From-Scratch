# LLM From Scratch (Transformer Implementation)

Proyek ini adalah implementasi **End-to-End Large Language Model (LLM) Training Pipeline** yang dibangun dari nol (from scratch) menggunakan PyTorch. Proyek ini dirancang untuk tujuan edukasi dan penelitian, dengan fokus pada pemahaman mendalam mengenai arsitektur **Transformer** dan proses pelatihan model bahasa.

Implementasi ini mengacu pada makalah seminal _"Attention Is All You Need"_ (Vaswani et al., 2017) namun disederhanakan menjadi model _Decoder-only_ (seperti GPT) untuk tugas _Causal Language Modeling_.

---

## ğŸ“‚ Struktur Proyek

Struktur folder disusun secara modular untuk memisahkan antara data, arsitektur model, dan logika pelatihan.

```text
llm-from-scratch/
â”‚
â”œâ”€â”€ data/                # Manajemen Dataset
â”‚   â”œâ”€â”€ raw/             # Dataset mentah (txt/csv)
â”‚   â”œâ”€â”€ cleaned/         # Dataset bersih setelah preprocessing
â”‚   â””â”€â”€ tokenized/       # Dataset yang sudah diubah menjadi token ID
â”‚
â”œâ”€â”€ tokenizer/           # Komponen Tokenisasi
â”‚   â”œâ”€â”€ train_tokenizer.py
â”‚   â””â”€â”€ tokenizer.model  # Model BPE yang sudah dilatih
â”‚
â”œâ”€â”€ model/               # Arsitektur Neural Network (Transformer)
â”‚   â”œâ”€â”€ embedding.py     # Token Embedding & Positional Encoding
â”‚   â”œâ”€â”€ attention.py     # Multi-Head Self-Attention
â”‚   â”œâ”€â”€ transformer.py   # Blok Decoder & Model Utama
â”‚   â””â”€â”€ lm_head.py       # Output Layer
â”‚
â”œâ”€â”€ training/            # Logika Pelatihan (Training Loop)
â”‚   â”œâ”€â”€ dataset.py       # PyTorch Dataset & DataLoader
â”‚   â”œâ”€â”€ train.py         # Script utama pelatihan
â”‚   â””â”€â”€ config.yaml      # Konfigurasi Hyperparameter
â”‚
â”œâ”€â”€ scripts/             # Utilitas Pengolahan Data
â”‚   â””â”€â”€ preprocess_data.py
â”‚
â”œâ”€â”€ inference/           # Pengujian Model
â”‚   â””â”€â”€ generate.py      # Script untuk generate teks
â”‚
â””â”€â”€ requirements.txt     # Daftar dependensi
```

---

## ğŸš€ Cara Menjalankan (Pipeline)

Ikuti langkah-langkah berikut secara berurutan untuk melatih model dari data mentah hingga siap digunakan.

### 0. Persiapan Lingkungan

Pastikan Python sudah terinstal, lalu instal library yang dibutuhkan:

```bash
pip install -r requirements.txt
```

### 1. Data Preparation (Stage 1)

Letakkan file dataset mentah Anda (misalnya `raw_corpus.txt`) di dalam folder `data/raw/`. Kemudian jalankan script cleaning:

```bash
python scripts/preprocess_data.py
```

_Output: `data/cleaned/cleaned_corpus.txt`_

### 2. Tokenizer Construction (Stage 2)

Latih tokenizer (BPE/Byte-Pair Encoding) menggunakan data yang sudah dibersihkan untuk membuat vocabulary.

```bash
python tokenizer/train_tokenizer.py
```

_Output: `tokenizer/tokenizer.model` dan `tokenizer/tokenizer.vocab`_

### 3. Training Model (Stage 3 & 4)

Mulai proses pelatihan. Anda dapat mengatur hyperparameter (seperti `batch_size`, `learning_rate`, `n_layer`) di file `training/config.yaml`.

```bash
python training/train.py
```

_Output: Checkpoint model akan disimpan di folder `checkpoints/`_

### 4. Inference / Text Generation (Stage 5)

Gunakan model yang sudah dilatih untuk menghasilkan teks baru berdasarkan prompt.

```bash
python inference/generate.py --prompt "Artificial Intelligence adalah"
```

---

## ğŸ§  Metodologi & Arsitektur

Proyek ini mengimplementasikan komponen-komponen kunci berikut:

1.  **Tokenization**: Menggunakan Byte-Pair Encoding (BPE) untuk menangani _out-of-vocabulary words_.
2.  **Embedding**: Input embedding ditambah dengan _Positional Encoding_ (sinusoidal).
3.  **Self-Attention**: Mekanisme _Scaled Dot-Product Attention_ dengan _Causal Masking_ (agar model tidak melihat masa depan).
4.  **Feed-Forward Network**: Dua layer linear dengan aktivasi ReLU/GELU.
5.  **Normalization**: Layer Normalization (Pre-Norm atau Post-Norm).

---

## ğŸ“ Catatan

- Pastikan Anda memiliki memori (RAM/VRAM) yang cukup jika melatih dengan dataset besar.
- Gunakan GPU (CUDA) untuk mempercepat proses pelatihan. Konfigurasi device diatur otomatis di dalam script.

---
